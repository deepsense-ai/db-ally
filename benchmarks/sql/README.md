# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following tasks:

- `IQL_VIEW` - measures correctness of SQL queries generated by the structured views.
- `SQL_VIEW` - measures correctness of SQL queries generated by the freeform views.


All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, only one configuration is available to run the suite against the `superhero` database. We plan to extend it to all databases in the set to cover all cases.

New PRs adding support for new databases from BIRD or SPIDER are welcome.

## Run benchmarks

### Usage

Before starting, download the `superhero.sqlite` database file from [BIRD](https://bird-bench.github.io/), change its extension to `*.db` and place it in the `data/` folder.

Run the whole suite on the `superhero` database:

```bash
python bench.py --multirun setup=iql-view,sql-view data=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py setup=iql-view
python bench.py --multirun setup=iql-view,sql-view
```

Compare IQL generation performance on multiple LLMs:

```bash
python bench.py --multirun setup=iql-view setup/llm=gpt-3.5-turbo,claude-3.5-sonnet
```

### Log to Neptune

Before running the suite with Neptune, configure the following environment variables:

```bash
export NEPTUNE_API_TOKEN="API_TOKEN"
export NEPTUNE_PROJECT="WORKSPACE_NAME/PROJECT_NAME"
```

Export evaluation results to Neptune:

```bash
python bench.py setup=iql-view neptune=True
```

## Metrics

This suite computes following metrics:

- `EM_IQL` - ratio of predicated IQL queries that are identical to the ground truth ones.
- `VAL_IQL` - ratio of valid IQL queries.
- `UNSUPP_IQL` - ratio of unsupported IQL queries.
- `EM_SQL` - ratio of predicated SQL queries that are identical to the ground truth ones.
- ...

## Add new dataset

In order to run this suite against your own dataset, upload it to [Hugging Face](https://huggingface.co) and make sure the data is in the format expected by the evaluation pipeline.

Evaluation dataset required fields:

- `question` - natural langugage SQL prompt
- `sql` - SQL corresponding to the SQL prompt
- `view` - view name corresponding to the SQL prompt
- `iql_filters` - IQL filters corresponding to the SQL prompt
- `iql_aggregation` - IQL agrregation corresponding to the SQL prompt
- `difficulty` - SQL code difficulty label
- `db_id` - database identifier

In addition, add a database file in the `data/` folder and create a structure and freeform view in the `bench.views` module for evaluation.
