# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following view setups:

- `structured` - measures correctness of SQL queries generated by the collection with structured views only.
- `freeform` - measures correctness of SQL queries generated by the collection with freeform views only.
- `mixed` - measures correctness of SQL queries generated by the collection with both structured and freeform views.

All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, only one configuration is available to run the suite against the `superhero` database. We plan to extend it to all databases in the set to cover all cases.

New PRs adding support for new databases from BIRD or SPIDER are welcome.

## Run benchmarks

### Usage

Before starting, download the `superhero.sqlite` database file from [BIRD](https://bird-bench.github.io/), change its extension to `*.db` and place it in the `data/` folder.

Run the whole suite on the `superhero` database:

```bash
python bench.py --multirun setup=iql,sql,mixed data=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py setup=iql
python bench.py --multirun setup=iql,sql
```

Change views for the setup:

```bash
python bench.py setup=iql setup/views=new-db/structured
```

Compare IQL generation performance on multiple LLMs:

```bash
python bench.py --multirun setup=iql llm=gpt,claude
```

### Log to Neptune

Before running the suite with Neptune, configure the following environment variables:

```bash
export NEPTUNE_API_TOKEN="API_TOKEN"
export NEPTUNE_PROJECT="WORKSPACE_NAME/PROJECT_NAME"
```

Export evaluation results to Neptune:

```bash
python bench.py setup=iql neptune=True
```

## Run tests

```bash
python -m pytest
```

## Metrics

This suite computes following metrics:

- `EM_IQL` - ratio of predicated IQL queries that are identical to the ground truth ones.
- `VAL_IQL` - ratio of valid IQL queries.
- `UNSUPP_IQL` - ratio of unsupported IQL queries.
- `HAL_IQL` - ratio of hallucinated IQL queries.
- `EM_SQL` - ratio of predicated SQL queries that are identical to the ground truth ones.
- ...

## Add new dataset

In order to run this suite against your own dataset, upload it to [Hugging Face](https://huggingface.co) and make sure the data is in the format expected by the evaluation pipeline.

Evaluation dataset required fields:

- `question` - natural langugage SQL prompt
- `sql` - SQL corresponding to the SQL prompt
- `iql` - IQL corresponding to the SQL prompt
- `difficulty` - SQL code difficulty label
- `db_id` - database identifier

In addition, add a database file in the `data/` folder and create a structure and freeform view in the `bench.views` module for evaluation.
