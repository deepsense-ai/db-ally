# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following components:

- `COLLECTION` - measures correctness of SQL queries generated by the collection.
- `IQL_VIEW` - measures correctness of SQL queries generated by the structured views.
- `SQL_VIEW` - measures correctness of SQL queries generated by the freeform views.

All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, one configuration is available to run the suite against the `superhero` database.

## Run benchmarks

### Usage

Before starting, download the `superhero.sqlite` database file from [BIRD](https://bird-bench.github.io/), change its extension to `*.db` and place it in the `data/` folder.

Run the whole suite on the `superhero` database with `gpt-3.5-turbo`:

```bash
python bench.py --multirun setup=iql-view,sql-view,collection data=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py setup=iql-view
python bench.py --multirun setup=iql-view,sql-view
```

Compare IQL/SQL generation performance on multiple LLMs:

```bash
python bench.py --multirun setup=iql-view setup/llm=gpt-3.5-turbo,claude-3.5-sonnet
python bench.py --multirun setup=sql-view setup/llm=gpt-3.5-turbo,claude-3.5-sonnet
```

For the `collection` steup, you need to specify models for both the view selection and the IQL generation step:

```bash
python bench.py --multirun \
    setup=collection \
    setup/llm@setup.selector_llm=gpt-3.5-turbo,claude-3.5-sonnet \
    setup/llm@setup.generator_llm=gpt-3.5-turbo,claude-3.5-sonnet
```

### Log to Neptune

Before running the suite with Neptune, configure the following environment variables:

```bash
export NEPTUNE_API_TOKEN="API_TOKEN"
export NEPTUNE_PROJECT="WORKSPACE_NAME/PROJECT_NAME"
```

Export evaluation results to Neptune:

```bash
python bench.py setup=iql-view neptune=True
```

## Run tests

```bash
python -m pytest
```
