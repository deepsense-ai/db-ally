# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following tasks:

- `E2E` - measures correctness of rows returned from the database by db-ally.
- `IQL` - measures correctness of IQL queries generated by structured views.
- `SQL` - measures correctness of SQL queries generated by freeform views.

All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, only one configuration is available to run the suite against the `superhero` database. We plan to extend it to all databases in the set to cover all cases.

Any new PRs adding support for new databases from BIRD or SPIDER are welcome.

## Run benchmarks

Run the whole suite on the `superhero` database:

```bash
python bench.py task=iql,sql,e2e data=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py task=e2e
python bench.py task=iql,sql
```

Compare IQL generation performance on multiple LLMs:

```bash
python bench.py --multirun task=iql llm=gpt,claude
```

## Run tests

```bash
python -m pytest
```

## Metrics

This suite computes following metrics:

- `exact_match` - ratio of predicated queries that are identical to the ground truth ones.
- `exact_match` - estimates the pass@k metric for code synthesis.

## Add new dataset

In order to run this suite against you own dataset, upload your dataset to [Hugging Face](https://huggingface.co) and make sure the data is in the format expected by the evaluation pipeline.

Evaluation dataset required fields:

- `question` - natural langugage SQL prompt
- `sql` - SQL corresponding to the SQL prompt
- `iql` - IQL corresponding to the SQL prompt
- `difficulty` - SQL code difficulty label
- `db_id` - database identifier


Additionaly, you need to create approprite structure and freeform view for downstream tasks