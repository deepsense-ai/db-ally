# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following components:

- `COLLECTION` - measures correctness of SQL queries generated by the collection in a multi-view setup.
- `IQL-VIEW` - measures correctness of SQL queries generated by structured views.
- `SQL-VIEW` - measures correctness of SQL queries generated by freeform views.

All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, only one configuration is available to run the suite against the `superhero` database. We plan to extend it to all databases in the set to cover all cases.

New PRs adding support for new databases from BIRD or SPIDER are welcome.

## Run benchmarks

### Usage

Before starting, download the `superhero.sqlite` database file from [BIRD](https://bird-bench.github.io/) and change its extension to `*.db`, place it in the `data/` folder.

Run the whole suite on the `superhero` database:

```bash
python bench.py --multirun component=iql-view,sql-view,collection data=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py component=iql-view
python bench.py --multirun component=iql-view,sql-view
```

Compare IQL generation performance on multiple LLMs:

```bash
python bench.py --multirun component=iql-view llm=gpt,claude
```

### Log to Neptune

Before running the suite with Neptune, configure the following environment variables:

```bash
export NEPTUNE_API_TOKEN="API_TOKEN"
export NEPTUNE_PROJECT="WORKSPACE_NAME/PROJECT_NAME"
```

Export evaluation results to Neptune:

```bash
python bench.py component=iql-view neptune=True
```

## Run tests

```bash
python -m pytest
```

## Metrics

This suite computes following metrics:

- `EM_IQL` - ratio of predicated IQL queries that are identical to the ground truth ones.
- `VAL_IQL` - ratio of valid IQL queries.
- `UNSUPP_IQL` - ratio of unsupported IQL queries.
- `HAL_IQL` - ratio of hallucinated IQL queries.
- `EM_SQL` - ratio of predicated SQL queries that are identical to the ground truth ones.
- ...

## Add new dataset

In order to run this suite against your own dataset, upload it to [Hugging Face](https://huggingface.co) and make sure the data is in the format expected by the evaluation pipeline.

Evaluation dataset required fields:

- `question` - natural langugage SQL prompt
- `sql` - SQL corresponding to the SQL prompt
- `iql` - IQL corresponding to the SQL prompt
- `difficulty` - SQL code difficulty label
- `db_id` - database identifier

In addition, add a database file in the `data/` folder and create a structure and freeform view in the `bench.views` module for evaluation.
