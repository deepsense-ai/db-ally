# SQL benchmarks

This folder contains benchmarks for querying SQL databases with db-ally. This suite evaluates the following tasks:

- `E2E` - measures correctness of rows returned from the database by db-ally.
- `Text2IQL` - measures correctness of IQL queries generated by structured views.
- `Text2SQL` - measures correctness of SQL queries generated by freeform views.

All benchmarks are run on a dev split of the [BIRD](https://bird-bench.github.io/) dataset. For now, only one configuration is available to run the suite against the `superhero` database. We plan to extend it to all databases in the set to cover all cases.

Any new PRs adding support for new databases from BIRD are welcome.

## Run benchmarks

Run the whole suite on the `superhero` database:

```bash
python bench.py e2e=superhero iql=superhero sql=superhero
```

You can also run each evaluation separately or in subgroups:

```bash
python bench.py e2e=superhero ...
```

## Run tests

```bash
python -m pytest
```

## Metrics

Each task computes following metrics:

tbd

## Custom evaluation dataset

In order to run this suite against you own dataset, upload your dataset to [Hugging Face](https://huggingface.co) and make sure the data is in the format expected by the evaluation pipeline.

Evaluation dataset required fields:

- `question` - natural langugage SQL prompt
- `sql` - SQL corresponding to the SQL prompt
- `iql` - IQL corresponding to the SQL prompt
- `difficulty` - SQL code difficulty label
- `db_id` - database identifier
