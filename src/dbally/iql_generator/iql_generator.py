import copy
from typing import Callable, List, Optional, Tuple, TypeVar

from dbally.audit.event_tracker import EventTracker
from dbally.iql_generator.iql_prompt_template import IQLPromptTemplate, default_iql_template
from dbally.llms.base import LLM
from dbally.llms.clients.base import LLMOptions
from dbally.views.exposed_functions import ExposedFunction


class IQLGenerator:
    """
    Class used to generate IQL from natural language question.

    In db-ally, LLM uses IQL (Intermediate Query Language) to express complex queries in a simplified way.
    The class used to generate IQL from natural language query is `IQLGenerator`.

    IQL generation is done using the method `self.generate_iql`.
    It uses LLM to generate text-based responses, passing in the prompt template, formatted filters, and user question.
    """

    _ERROR_MSG_PREFIX = "Unfortunately, generated IQL is not valid. Please try again, \
                        generation of correct IQL is very important. Below you have errors generated by the system: \n"

    TException = TypeVar("TException", bound=Exception)

    def __init__(
        self,
        llm: LLM,
        prompt_template: Optional[IQLPromptTemplate] = None,
        promptify_view: Optional[Callable] = None,
    ) -> None:
        """
        Args:
            llm: LLM used to generate IQL
            prompt_template: If not provided by the users is set to `default_iql_template`
            promptify_view: Function formatting filters for prompt
        """
        self._llm = llm
        self._prompt_template = prompt_template or copy.deepcopy(default_iql_template)
        self._promptify_view = promptify_view or _promptify_filters

    async def generate_iql(
        self,
        filters: List[ExposedFunction],
        question: str,
        event_tracker: EventTracker,
        conversation: Optional[IQLPromptTemplate] = None,
        llm_options: Optional[LLMOptions] = None,
    ) -> Tuple[str, IQLPromptTemplate]:
        """
        Uses LLM to generate IQL in text form

        Args:
            question: user question
            filters: list of filters exposed by the view
            event_tracker: event store used to audit the generation process
            conversation: conversation to be continued
            llm_options: options to use for the LLM client

        Returns:
            IQL - iql generated based on the user question
        """
        filters_for_prompt = self._promptify_view(filters)

        template = conversation or self._prompt_template

        llm_response = await self._llm.generate_text(
            template=template,
            fmt={"filters": filters_for_prompt, "question": question},
            event_tracker=event_tracker,
            options=llm_options,
        )

        iql_filters = self._prompt_template.llm_response_parser(llm_response)

        if conversation is None:
            conversation = self._prompt_template

        conversation = conversation.add_assistant_message(content=llm_response)

        return iql_filters, conversation

    def add_error_msg(self, conversation: IQLPromptTemplate, errors: List[TException]) -> IQLPromptTemplate:
        """
        Appends to the conversation error messages returned due to the invalid IQL generated by the LLM.

        Args:
            conversation (IQLPromptTemplate): conversation containing current IQL generation trace
            errors (List[Exception]): errors to be appended

        Returns:
            IQLPromptTemplate: Conversation extended with errors
        """

        msg = self._ERROR_MSG_PREFIX
        for error in errors:
            msg += str(error) + "\n"

        return conversation.add_user_message(content=msg)


def _promptify_filters(
    filters: List[ExposedFunction],
) -> str:
    """
    Formats filters for prompt

    Args:
        filters: list of filters exposed by the view

    Returns:
        filters_for_prompt: filters formatted for prompt
    """
    filters_for_prompt = "\n".join([str(filter) for filter in filters])
    return filters_for_prompt
