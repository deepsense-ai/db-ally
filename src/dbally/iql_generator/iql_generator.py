from typing import List, Optional, Tuple, TypeVar

from dbally.audit.event_tracker import EventTracker
from dbally.iql_generator.iql_format import AbstractIQLInputFormatter
from dbally.iql_generator.iql_prompt_template import IQLPromptTemplate, default_iql_template  # noqa
from dbally.llms.base import LLM
from dbally.llms.clients.base import LLMOptions


class IQLGenerator:
    """
    Class used to generate IQL from natural language question.

    In db-ally, LLM uses IQL (Intermediate Query Language) to express complex queries in a simplified way.
    The class used to generate IQL from natural language query is `IQLGenerator`.

    IQL generation is done using the method `self.generate_iql`.
    It uses LLM to generate text-based responses, passing in the prompt template, formatted filters, and user question.
    """

    _ERROR_MSG_PREFIX = "Unfortunately, generated IQL is not valid. Please try again, \
                        generation of correct IQL is very important. Below you have errors generated by the system: \n"

    TException = TypeVar("TException", bound=Exception)

    def __init__(self, llm: LLM) -> None:
        """
        Args:
            llm: LLM used to generate IQL
        """
        self._llm = llm

    async def generate_iql(
        self,
        input_formatter: AbstractIQLInputFormatter,
        event_tracker: EventTracker,
        conversation: Optional[IQLPromptTemplate] = None,
        llm_options: Optional[LLMOptions] = None,
    ) -> Tuple[str, IQLPromptTemplate]:
        """
        Uses LLM to generate IQL in text form

        Args:
            input_formatter: formatter used to prepare prompt arguments dictionary
            event_tracker: event store used to audit the generation process
            conversation: conversation to be continued
            llm_options: options to use for the LLM client

        Returns:
            IQL - iql generated based on the user question
        """

        conversation, fmt = input_formatter(conversation or default_iql_template)

        llm_response = await self._llm.generate_text(
            template=conversation,
            fmt=fmt,
            event_tracker=event_tracker,
            options=llm_options,
        )

        iql_filters = conversation.llm_response_parser(llm_response)

        conversation = conversation.add_assistant_message(content=llm_response)

        return iql_filters, conversation

    def add_error_msg(self, conversation: IQLPromptTemplate, errors: List[TException]) -> IQLPromptTemplate:
        """
        Appends to the conversation error messages returned due to the invalid IQL generated by the LLM.

        Args:
            conversation (IQLPromptTemplate): conversation containing current IQL generation trace
            errors (List[Exception]): errors to be appended

        Returns:
            IQLPromptTemplate: Conversation extended with errors
        """

        msg = self._ERROR_MSG_PREFIX
        for error in errors:
            msg += str(error) + "\n"

        return conversation.add_user_message(content=msg)
