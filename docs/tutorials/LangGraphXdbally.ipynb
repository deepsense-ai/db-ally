{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnzuFhmmofkx"
   },
   "source": [
    "# Database querying system with the use of db-ally\n",
    "\n",
    "In this guide, we will build a database querying agentic system based on the [db-ally package](https://db-ally.deepsense.ai/). Along the way, we will learn about a [Supervisor Agent](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb), dynamic State modification, and the [human-in-the-loop component](https://github.com/langchain-ai/langgraph/blob/main/examples/human-in-the-loop.ipynb)\n",
    "\n",
    "Our goal is to build a system that looks like this:\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1McIouHQR9ITYQOmExCtzSeL8Iz2Nyhw1\" alt=\"Agents diagram\" width=\"700\" height=\"auto\">\n",
    "\n",
    "Agents have the following duties:\n",
    "\n",
    "**Configuration Agent**: based on the user query it changes configurations that are used by other agents\n",
    "\n",
    "**db-ally Agent**: given the question in the natural language it queries the given database and responds accordingly\n",
    "\n",
    "**human-in-the-loop**: Sometimes we may miss some kind of information to formulate a query, in such a case we simply ask the user to provide more.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVsS87D4gfna"
   },
   "outputs": [],
   "source": [
    "!pip install -U dbally[litellm,langsmith] langgraph langchain langchain_openai langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrKhnrJ2sq-U"
   },
   "source": [
    "We will use the OpenAI API. We have to set the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yRo8Wy8rgDD2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RY_wBUksxdE"
   },
   "source": [
    "Finally, we want to query a database so we should have one. So, let's download a [dummy HR Recruiting database](https://drive.google.com/file/d/1A5yPt-pIyXGV94c6cMIkMf8AhiP6Nnq6/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y0FGQYw_gu2D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-18 16:02:02--  https://drive.google.com/uc?export=download&id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.203.206, 2a00:1450:401b:810::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.203.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs&export=download [following]\n",
      "--2024-04-18 16:02:02--  https://drive.usercontent.google.com/download?id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.75.1, 2a00:1450:401b:800::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.75.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24576 (24K) [application/octet-stream]\n",
      "Saving to: ‘recruitment.db’\n",
      "\n",
      "recruitment.db      100%[===================>]  24,00K  --.-KB/s    in 0,01s   \n",
      "\n",
      "2024-04-18 16:02:03 (1,69 MB/s) - ‘recruitment.db’ saved [24576/24576]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O recruitment.db 'https://drive.google.com/uc?export=download&id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBIXu6yXvgU8"
   },
   "source": [
    "and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2MdjtevmkNOR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181984/2090593471.py:8: SADeprecationWarning: The AutomapBase.prepare.reflect parameter is deprecated and will be removed in a future release.  Reflection is enabled when AutomapBase.prepare.autoload_with is passed.\n",
      "  RECRUITMENT_MODEL.prepare(autoload_with=ENGINE, reflect=True)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import select\n",
    "\n",
    "ENGINE = create_engine(\"sqlite:///recruitment.db\")\n",
    "\n",
    "RECRUITMENT_MODEL = automap_base()\n",
    "RECRUITMENT_MODEL.prepare(autoload_with=ENGINE, reflect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2Wj95dlviN1"
   },
   "source": [
    "The next step is to provide db-ally access to the database via [views](https://db-ally.deepsense.ai/concepts/views/) and [filters](https://db-ally.deepsense.ai/concepts/iql/). Here, we allowed db-ally to answer questions about offered job positions and the experience of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HpZrSDzukO_6"
   },
   "outputs": [],
   "source": [
    "from dbally import SqlAlchemyBaseView, decorators\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "class JobOfferView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about job offers.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.offer)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_job_offers_by_position(self, position: str) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.offer.position == position\n",
    "\n",
    "\n",
    "class CandidateView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about candiates.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.candidate)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_candidates_by_experience(self, years: int) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.candidate.years_of_experience >= years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8mbhIp1u8DT"
   },
   "source": [
    "Consequently, we need to create a [collection](https://db-ally.deepsense.ai/concepts/collections/) and register previously created views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wv2GWXkPkQ-5"
   },
   "outputs": [],
   "source": [
    "import dbally\n",
    "from dbally.llms.litellm import LiteLLM\n",
    "\n",
    "recruitment_db = dbally.create_collection(\"recruitment\", llm=LiteLLM())\n",
    "recruitment_db.add(JobOfferView, lambda: JobOfferView(ENGINE))\n",
    "recruitment_db.add(CandidateView, lambda: CandidateView(ENGINE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M3Gj42B4qlG"
   },
   "source": [
    "## Agents State\n",
    "\n",
    "After creating the collection, we move and define the State of the entire system. In this case, it contains three components:\n",
    "1. List of messages exchanged between agents\n",
    "2. Configuration of db-ally where we can set [the format of the answer](https://db-ally.deepsense.ai/concepts/nl_responder/), the used collection, and whether to [log runs to langsmith](https://db-ally.deepsense.ai/how-to/log_runs_to_langsmith/).\n",
    "3. Information about which agent should be called now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xi8E2rGW4sNO"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "COLLECTIONS = {\"Recruitment\": recruitment_db, \"Benefits\": recruitment_db}\n",
    "\n",
    "\n",
    "AvailableCollections = enum.Enum(\"AvailableCollections\", list(COLLECTIONS.keys()))\n",
    "\n",
    "\n",
    "class DballyConfig(BaseModel):\n",
    "    \"\"\"Modifies the configuration used by the db-ally engine to generate a response\"\"\"\n",
    "\n",
    "    use_nl_responses: bool = Field(default=None, description=\"Whether or not to use natural language response\")\n",
    "    used_collection: AvailableCollections = Field(\n",
    "        default=AvailableCollections.Recruitment, description=\"Which collection should be used\"\n",
    "    )\n",
    "    log_to_langsmith: bool = Field(default=None, description=\"Whether to log conversations to the langsmith system\")\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    dbally_config: DballyConfig\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOWjl2dO-Kve"
   },
   "source": [
    "To start the interaction, we will need to provide an initial configuration and state.\n",
    "\n",
    "Below you can see, how the state and configuration may look at some point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xAxnMnb4tuaR"
   },
   "outputs": [],
   "source": [
    "example_dbally_config = DballyConfig(\n",
    "    use_nl_responses=False, used_collection=AvailableCollections.Recruitment, log_to_langsmith=False\n",
    ")\n",
    "\n",
    "example_state = AgentState(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"Do we have an offer for Software Engineer?\", name=\"User\"),\n",
    "        AIMessage(\n",
    "            content=\"Yes, we have 4 offers for Software Engineers, do you want to learn more about them?\",\n",
    "            name=\"db-ally\",\n",
    "        ),\n",
    "    ],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhSVpvtW34fO"
   },
   "source": [
    "## db-ally agent\n",
    "\n",
    "Now, we create the first agent. This one performs an asynchronous call to db-ally based on the last message and the current configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jwc8WaTn35hm"
   },
   "outputs": [],
   "source": [
    "from dbally.iql_generator.iql_prompt_template import UnsupportedQueryError\n",
    "\n",
    "\n",
    "async def call_dbally(state: AgentState):\n",
    "    message = state[\"messages\"][-1].content\n",
    "    dbally_config = state[\"dbally_config\"]\n",
    "\n",
    "    try:\n",
    "        result = await recruitment_db.ask(message, return_natural_response=dbally_config.use_nl_responses)\n",
    "\n",
    "        if result.textual_response is not None:\n",
    "            return {\"messages\": [AIMessage(content=result.textual_response, name=\"db-ally\")]}\n",
    "        else:\n",
    "            return {\"messages\": [AIMessage(content=str(result.results), name=\"db-ally\")]}\n",
    "    except UnsupportedQueryError:\n",
    "        return {\"messages\": [AIMessage(contest=\"database master can't answer this question\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eDgVb1lAebX"
   },
   "source": [
    "Here you can see how the agent will respond, given a particular state. Because we don't want natural language responses at this particular moment, you should see a list of database records, each formatted as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q8UxTrpDt2ap"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"[{'id': 5, 'company': 'HuggingFace', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$80000'}, {'id': 10, 'company': 'OpenAI', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$85000'}, {'id': 15, 'company': 'Google', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$90000'}, {'id': 20, 'company': 'Apple', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$100000'}, {'id': 25, 'company': 'Microsoft', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$110000'}, {'id': 30, 'company': 'Google', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$115000'}, {'id': 35, 'company': 'OpenAI', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$120000'}, {'id': 40, 'company': 'HuggingFace', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$125000'}, {'id': 45, 'company': 'Apple', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$130000'}, {'id': 50, 'company': 'Microsoft', 'position': 'Software Engineer', 'excpected_years_of_experience': 3, 'salary': '$135000'}]\", name='db-ally')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"Do we have an offer for Software Engineer?\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "await call_dbally(example_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc1G4TVfytQE"
   },
   "source": [
    "## Configuration Agent\n",
    "\n",
    "Now we move to the configuration agent that given the user query sets up the appropriate configuration.\n",
    "\n",
    "For this purpose, we use the [LangChain tool calling](https://python.langchain.com/docs/modules/model_io/chat/function_calling/) which supports pasing schema using Pydantic. So we can our `DballyConfig` directly as the `tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eNQIsKiwn47q"
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "config_llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "config_llm = config_llm.bind_tools([DballyConfig]) | PydanticToolsParser(tools=[DballyConfig])\n",
    "\n",
    "\n",
    "def change_dbally_config(state: AgentState):\n",
    "    message = state[\"messages\"][-1].content\n",
    "    config_modification = config_llm.invoke(message)[0]\n",
    "\n",
    "    new_config = copy(state[\"dbally_config\"])\n",
    "\n",
    "    if config_modification.use_nl_responses is not None:\n",
    "        new_config.use_nl_responses = config_modification.use_nl_responses\n",
    "\n",
    "    if config_modification.used_collection is not None:\n",
    "        new_config.used_collection = config_modification.used_collection\n",
    "\n",
    "    if config_modification.log_to_langsmith is not None:\n",
    "        new_config.log_to_langsmith = config_modification.log_to_langsmith\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Configuration adjusted. Ask human what to do now.\", name=\"change_dbally_config\")\n",
    "        ],\n",
    "        \"dbally_config\": new_config,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTOJO2lvDmCc"
   },
   "source": [
    "Under the hood, DballyConfig is parsed into [OpenAI function calling format](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cVfLjDv7DzCY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DballyConfig',\n",
       " 'description': 'Modifies the configuration used by the db-ally engine to generate a response',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'use_nl_responses': {'description': 'Whether or not to use natural language response',\n",
       "    'type': 'boolean'},\n",
       "   'used_collection': {'description': 'Which collection should be used',\n",
       "    'default': 1,\n",
       "    'allOf': [{'title': 'AvailableCollections',\n",
       "      'description': 'An enumeration.',\n",
       "      'enum': [1, 2]}]},\n",
       "   'log_to_langsmith': {'description': 'Whether to log conversations to the langsmith system',\n",
       "    'type': 'boolean'}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "convert_to_openai_function(DballyConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nofWMAEJET7M"
   },
   "source": [
    "Here you can see how this agent will behave when it is executed with the state below. Take a look at the dbally_config, `use_nl_responses` should be set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G0G_DB9ahPhe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Configuration adjusted. Ask human what to do now.', name='change_dbally_config')],\n",
       " 'dbally_config': DballyConfig(use_nl_responses=True, used_collection=<AvailableCollections.Recruitment: 1>, log_to_langsmith=False)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "change_dbally_config(example_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVmGWcSII7tI"
   },
   "source": [
    "## Human-in-the-loop Agent\n",
    "\n",
    "Now, we create an agent that will ask our users, if they need something more, or if they can provide additional information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p3cm1h-lJzWF"
   },
   "outputs": [],
   "source": [
    "def human_in_the_loop(state: AgentState):\n",
    "    response = input(f\"What's next?\")\n",
    "    return {\"messages\": [HumanMessage(content=response, name=\"User\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0GHXIhlyqje"
   },
   "source": [
    "## Supervisor Agent\n",
    "\n",
    "The last building block of our system is a supervisor who decides which agent is the most appropriate given the current situation.\n",
    "\n",
    "To create it, we:\n",
    "1. Craft a system prompt that explains the supervisor's task\n",
    "2. Utilize function calling. Look carefully at the docstring of the `SelectedRole` it should explain well which agent to use when\n",
    "3. Build a prompt template. Observe that we provide the entire history of the conversation.\n",
    "4. Create a chain\n",
    "5. Write a function that executes the chain and updates the State with the next agent to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DNMPvqB_ypxt"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Configuration Setter\", \"Database master\", \"human\", \"finish\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status.\"\n",
    ")\n",
    "\n",
    "AvailableAgents = enum.Enum(\"AvailableAgents\", members)\n",
    "\n",
    "\n",
    "class SelectedRole(BaseModel):\n",
    "    \"\"\"Select next agent to be used in the system. Use:\n",
    "    1. Configuration Setter if human asks to change something in the answer, or you deduced it is necessary\n",
    "    2. Database master: to gather information necessary to answer the question\n",
    "    3. human: When you need an additional input to proceed\n",
    "    4. finish: If human is satisfied with the answer\"\"\"\n",
    "\n",
    "    next: AvailableAgents\n",
    "\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ").partial(members=\", \".join(members))\n",
    "\n",
    "supervisor_llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    supervisor_prompt | supervisor_llm.bind_tools([SelectedRole]) | PydanticToolsParser(tools=[SelectedRole])\n",
    ")\n",
    "\n",
    "\n",
    "def call_supervisor(state: AgentState):\n",
    "    response = supervisor_chain.invoke(state)[0]\n",
    "    return {\"next\": response.next.name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTNjUqAiGuUf"
   },
   "source": [
    "Let's verify that the agent behaves correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qpdX_tYV0sa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next': 'Configuration Setter'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "call_supervisor(example_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVPa1jjH38-P"
   },
   "source": [
    "At this moment, we have all the building blocks. The last thing to do is to connect them into a graph. So, we:\n",
    "\n",
    "1. Create a graph and pass a `Schema of the State`\n",
    "2. Define all nodes - agents that will cooperate.\n",
    "3. Connect them with edges. Look carefully at the conditional edge.\n",
    "4. Set supervisor to be the entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BAj1V_d8kdxC"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"supervisor\", call_supervisor)\n",
    "graph.add_node(\"dbally\", call_dbally)\n",
    "graph.add_node(\"change_config\", change_dbally_config)\n",
    "graph.add_node(\"human\", human_in_the_loop)\n",
    "\n",
    "conditional_map = {\n",
    "    \"Configuration Setter\": \"change_config\",\n",
    "    \"Database master\": \"dbally\",\n",
    "    \"human\": \"human\",\n",
    "    \"finish\": END,\n",
    "}\n",
    "\n",
    "graph.add_edge(\"change_config\", \"supervisor\")\n",
    "graph.add_edge(\"dbally\", \"supervisor\")\n",
    "graph.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "graph.add_edge(\"human\", \"supervisor\")\n",
    "\n",
    "graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFM_-13PvwGl"
   },
   "source": [
    "Nice! The last thing we should do is to test it.\n",
    "\n",
    "We start our conversation by providing configuration information. Next, the `configuration setter` should be selected. After that the user will be asked to provide the next instructions please. You try with a question **Do we have offers for Software Engineer?** This triggers the Database master, which answers using natural language which brings us to the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m7Zz08rRldad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'next': 'Configuration Setter'}\n",
      "{'messages': [HumanMessage(content='Configuration adjusted. Ask human what to do now.', name='change_dbally_config')], 'dbally_config': DballyConfig(use_nl_responses=True, used_collection=<AvailableCollections.Recruitment: 1>, log_to_langsmith=False)}\n",
      "{'next': 'human'}\n",
      "{'messages': [HumanMessage(content='Do we have any data scientists?', name='User')]}\n",
      "{'next': 'Database master'}\n",
      "{'messages': [AIMessage(content='Yes, we have two data scientists in the database. One is Emily Chen from Canada with 3 years of experience, and the other is Anushka Sharma from India with 5 years of experience.', name='db-ally')]}\n",
      "{'next': 'human'}\n",
      "{'messages': [HumanMessage(content=\"That's all I wanted\", name='User')]}\n",
      "{'next': 'finish'}\n"
     ]
    }
   ],
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "\n",
    "async for event in app.astream(example_state):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu_20yZ2IU-f"
   },
   "source": [
    "Congratulations! Together, we built an agentic system capable of querying the database, changing the configuration dynamically, and incorporating human-in-the-loop. Good job!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNxLgjx0dL+aLtYmZD6WjF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
