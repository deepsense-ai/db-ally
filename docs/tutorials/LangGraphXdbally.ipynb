{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMNxLgjx0dL+aLtYmZD6WjF"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Database querying system with the use of db-ally\n",
    "\n",
    "In this guide, we will build a database querying agentic system based on the [db-ally package](https://db-ally.deepsense.ai/). Along the way, we will learn about a [Supervisor Agent](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb), dynamic State modification, and the [human-in-the-loop component](https://github.com/langchain-ai/langgraph/blob/main/examples/human-in-the-loop.ipynb)\n",
    "\n",
    "Our goal is to build a system that looks like this:\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1McIouHQR9ITYQOmExCtzSeL8Iz2Nyhw1\" alt=\"Agents diagram\" width=\"700\" height=\"auto\">\n",
    "\n",
    "Agents have the following duties:\n",
    "\n",
    "**Configuration Agent**: based on the user query it changes configurations that are used by other agents\n",
    "\n",
    "**db-ally Agent**: given the question in the natural language it queries the given database and responds accordingly\n",
    "\n",
    "**human-in-the-loop**: Sometimes we may miss some kind of information to formulate a query, in such a case we simply ask the user to provide more.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install all dependencies"
   ],
   "metadata": {
    "id": "QnzuFhmmofkx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U dbally[openai] langgraph langchain langchain_openai langchain_experimental dbally[langsmith]"
   ],
   "metadata": {
    "id": "aVsS87D4gfna"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the OpenAI API. We have to set the API key"
   ],
   "metadata": {
    "id": "VrKhnrJ2sq-U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ],
   "metadata": {
    "id": "yRo8Wy8rgDD2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we want to query a database so we should have one. So, let's download a [dummy HR Recruiting database](https://drive.google.com/file/d/1A5yPt-pIyXGV94c6cMIkMf8AhiP6Nnq6/view?usp=drive_link)."
   ],
   "metadata": {
    "id": "2RY_wBUksxdE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -O recruitment.db 'https://drive.google.com/uc?export=download&id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs'"
   ],
   "metadata": {
    "id": "y0FGQYw_gu2D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "and build it."
   ],
   "metadata": {
    "id": "iBIXu6yXvgU8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import select\n",
    "\n",
    "ENGINE = create_engine(\"sqlite:///recruitment.db\")\n",
    "\n",
    "RECRUITMENT_MODEL = automap_base()\n",
    "RECRUITMENT_MODEL.prepare(autoload_with=ENGINE, reflect=True)"
   ],
   "metadata": {
    "id": "2MdjtevmkNOR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to provide db-ally access to the database via [views](https://db-ally.deepsense.ai/concepts/views/) and [filters](https://db-ally.deepsense.ai/concepts/iql/). Here, we allowed db-ally to answer questions about offered job positions and the experience of candidates."
   ],
   "metadata": {
    "id": "t2Wj95dlviN1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from dbally import SqlAlchemyBaseView, decorators\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "class JobOfferView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about job offers.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.offer)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_job_offers_by_position(self, position: str) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.offer.position == position\n",
    "\n",
    "\n",
    "class CandidateView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about candiates.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.candidate)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_candidates_by_experience(self, years: int) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.candidate.years_of_experience >= years"
   ],
   "metadata": {
    "id": "HpZrSDzukO_6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consequently, we need to create a [collection](https://db-ally.deepsense.ai/concepts/collections/) and register previously created views."
   ],
   "metadata": {
    "id": "z8mbhIp1u8DT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import dbally\n",
    "\n",
    "dbally.use_openai_llm()\n",
    "\n",
    "recruitment_db = dbally.create_collection(\"recruitment\")\n",
    "recruitment_db.add(JobOfferView, lambda: JobOfferView(ENGINE))\n",
    "recruitment_db.add(CandidateView, lambda: CandidateView(ENGINE))"
   ],
   "metadata": {
    "id": "Wv2GWXkPkQ-5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agents State\n",
    "\n",
    "After creating the collection, we move and define the State of the entire system. In this case, it contains three components:\n",
    "1. List of messages exchanged between agents\n",
    "2. Configuration of db-ally where we can set [the format of the answer](https://db-ally.deepsense.ai/concepts/nl_responder/), the used collection, and whether to [log runs to langsmith](https://db-ally.deepsense.ai/how-to/log_runs_to_langsmith/).\n",
    "3. Information about which agent should be called now."
   ],
   "metadata": {
    "id": "7M3Gj42B4qlG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import enum\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "COLLECTIONS = {\"Recruitment\": recruitment_db, \"Benefits\": recruitment_db}\n",
    "\n",
    "\n",
    "AvailableCollections = enum.Enum(\"AvailableCollections\", list(COLLECTIONS.keys()))\n",
    "\n",
    "\n",
    "class DballyConfig(BaseModel):\n",
    "    \"\"\"Modifies the configuration used by the db-ally engine to generate a response\"\"\"\n",
    "\n",
    "    use_nl_responses: bool = Field(default=None, description=\"Whether or not to use natural language response\")\n",
    "    used_collection: AvailableCollections = Field(\n",
    "        default=AvailableCollections.Recruitment, description=\"Which collection should be used\"\n",
    "    )\n",
    "    log_to_langsmith: bool = Field(default=None, description=\"Whether to log conversations to the langsmith system\")\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    dbally_config: DballyConfig\n",
    "    next: str"
   ],
   "metadata": {
    "id": "xi8E2rGW4sNO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start the interaction, we will need to provide an initial configuration and state.\n",
    "\n",
    "Below you can see, how the state and configuration may look at some point in time."
   ],
   "metadata": {
    "id": "yOWjl2dO-Kve"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_dbally_config = DballyConfig(\n",
    "    use_nl_responses=False, used_collection=AvailableCollections.Recruitment, log_to_langsmith=False\n",
    ")\n",
    "\n",
    "example_state = AgentState(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"Do we have an offer for Software Engineer?\", name=\"User\"),\n",
    "        AIMessage(\n",
    "            content=\"Yes, we have 4 offers for Software Engineers, do you want to learn more about them?\",\n",
    "            name=\"db-ally\",\n",
    "        ),\n",
    "    ],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"human\",\n",
    ")"
   ],
   "metadata": {
    "id": "xAxnMnb4tuaR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## db-ally agent\n",
    "\n",
    "Now, we create the first agent. This one performs an asynchronous call to db-ally based on the last message and the current configuration.\n"
   ],
   "metadata": {
    "id": "LhSVpvtW34fO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "async def call_dbally(state: AgentState):\n",
    "    message = state[\"messages\"][-1].content\n",
    "    dbally_config = state[\"dbally_config\"]\n",
    "\n",
    "    # if dbally_config.log_to_langsmith:\n",
    "    #   dbally.use_event_handler(LangSmithEventHandler())\n",
    "\n",
    "    used_collection = COLLECTIONS[dbally_config.used_collection.name]\n",
    "\n",
    "    result = await recruitment_db.ask(message, return_natural_response=dbally_config.use_nl_responses)\n",
    "\n",
    "    if result.textual_response is not None:\n",
    "        return {\"messages\": [HumanMessage(content=result.textual_response, name=\"db-ally\")]}\n",
    "    else:\n",
    "        return {\"messages\": [HumanMessage(content=str(result.results), name=\"db-ally\")]}"
   ],
   "metadata": {
    "id": "jwc8WaTn35hm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here you can see how the agent will respond, given a particular state. Because we don't want natural language responses at this particular moment, you should see a list of database records, each formatted as a dictionary."
   ],
   "metadata": {
    "id": "6eDgVb1lAebX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"Do we have an offer for Software Engineer?\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "await call_dbally(example_state)"
   ],
   "metadata": {
    "id": "q8UxTrpDt2ap"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration Agent\n",
    "\n",
    "Now we move to the configuration agent that given the user query sets up the appropriate configuration.\n",
    "\n",
    "For this purpose, we use the [LangChain tool calling](https://python.langchain.com/docs/modules/model_io/chat/function_calling/) which supports pasing schema using Pydantic. So we can our `DballyConfig` directly as the `tool`."
   ],
   "metadata": {
    "id": "Xc1G4TVfytQE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import copy\n",
    "\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "config_llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "config_llm = config_llm.bind_tools([DballyConfig]) | PydanticToolsParser(tools=[DballyConfig])\n",
    "\n",
    "\n",
    "def change_dbally_config(state: AgentState):\n",
    "    message = state[\"messages\"][-1].content\n",
    "    config_modification = config_llm.invoke(message)[0]\n",
    "\n",
    "    new_config = copy(state[\"dbally_config\"])\n",
    "\n",
    "    if config_modification.use_nl_responses is not None:\n",
    "        new_config.use_nl_responses = config_modification.use_nl_responses\n",
    "\n",
    "    if config_modification.used_collection is not None:\n",
    "        new_config.used_collection = config_modification.used_collection\n",
    "\n",
    "    if config_modification.log_to_langsmith is not None:\n",
    "        new_config.log_to_langsmith = config_modification.log_to_langsmith\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Configuration adjusted. Ask human what to do now.\", name=\"change_dbally_config\")\n",
    "        ],\n",
    "        \"dbally_config\": new_config,\n",
    "    }"
   ],
   "metadata": {
    "id": "eNQIsKiwn47q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under the hood, DballyConfig is parsed into [OpenAI function calling format](https://platform.openai.com/docs/guides/function-calling)."
   ],
   "metadata": {
    "id": "XTOJO2lvDmCc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "convert_to_openai_function(DballyConfig)"
   ],
   "metadata": {
    "id": "cVfLjDv7DzCY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here you can see how this agent will behave when it is executed with the state below. Take a look at the dbally_config, `use_nl_responses` should be set to `True`"
   ],
   "metadata": {
    "id": "nofWMAEJET7M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "change_dbally_config(example_state)"
   ],
   "metadata": {
    "id": "G0G_DB9ahPhe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human-in-the-loop Agent\n",
    "\n",
    "Now, we create an agent that will ask our users, if they need something more, or if they can provide additional information\n"
   ],
   "metadata": {
    "id": "OVmGWcSII7tI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def human_in_the_loop(state: AgentState):\n",
    "    response = input(f\"What's next?\")\n",
    "    return {\"messages\": [HumanMessage(content=response, name=\"User\")]}"
   ],
   "metadata": {
    "id": "p3cm1h-lJzWF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supervisor Agent\n",
    "\n",
    "The last building block of our system is a supervisor who decides which agent is the most appropriate given the current situation.\n",
    "\n",
    "To create it, we:\n",
    "1. Craft a system prompt that explains the supervisor's task\n",
    "2. Utilize function calling. Look carefully at the docstring of the `SelectedRole` it should explain well which agent to use when\n",
    "3. Build a prompt template. Observe that we provide the entire history of the conversation.\n",
    "4. Create a chain\n",
    "5. Write a function that executes the chain and updates the State with the next agent to operate."
   ],
   "metadata": {
    "id": "C0GHXIhlyqje"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Configuration Setter\", \"Database master\", \"human\", \"finish\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status.\"\n",
    ")\n",
    "\n",
    "AvailableAgents = enum.Enum(\"AvailableAgents\", members)\n",
    "\n",
    "\n",
    "class SelectedRole(BaseModel):\n",
    "    \"\"\"Select next agent to be used in the system. Use:\n",
    "    1. Configuration Setter if human asks to change something in the answer, or you deduced it is necessary\n",
    "    2. Database master: to gather information necessary to answer the question\n",
    "    3. human: When you need an additional input to proceed\n",
    "    4. finish: If human is satisfied with the answer\"\"\"\n",
    "\n",
    "    next: AvailableAgents\n",
    "\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ").partial(members=\", \".join(members))\n",
    "\n",
    "supervisor_llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    supervisor_prompt | supervisor_llm.bind_tools([SelectedRole]) | PydanticToolsParser(tools=[SelectedRole])\n",
    ")\n",
    "\n",
    "\n",
    "def call_supervisor(state: AgentState):\n",
    "    response = supervisor_chain.invoke(state)[0]\n",
    "    return {\"next\": response.next.name}"
   ],
   "metadata": {
    "id": "DNMPvqB_ypxt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's verify that the agent behaves correctly."
   ],
   "metadata": {
    "id": "eTNjUqAiGuUf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "call_supervisor(example_state)"
   ],
   "metadata": {
    "id": "qpdX_tYV0sa1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this moment, we have all the building blocks. The last thing to do is to connect them into a graph. So, we:\n",
    "\n",
    "1. Create a graph and pass a `Schema of the State`\n",
    "2. Define all nodes - agents that will cooperate.\n",
    "3. Connect them with edges. Look carefully at the conditional edge.\n",
    "4. Set supervisor to be the entry point"
   ],
   "metadata": {
    "id": "iVPa1jjH38-P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"supervisor\", call_supervisor)\n",
    "graph.add_node(\"dbally\", call_dbally)\n",
    "graph.add_node(\"change_config\", change_dbally_config)\n",
    "graph.add_node(\"human\", human_in_the_loop)\n",
    "\n",
    "conditional_map = {\n",
    "    \"Configuration Setter\": \"change_config\",\n",
    "    \"Database master\": \"dbally\",\n",
    "    \"human\": \"human\",\n",
    "    \"finish\": END,\n",
    "}\n",
    "\n",
    "graph.add_edge(\"change_config\", \"supervisor\")\n",
    "graph.add_edge(\"dbally\", \"supervisor\")\n",
    "graph.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "graph.add_edge(\"human\", \"supervisor\")\n",
    "\n",
    "graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "app = graph.compile()"
   ],
   "metadata": {
    "id": "BAj1V_d8kdxC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! The last thing we should do is to test it.\n",
    "\n",
    "We start our conversation by providing configuration information. Next, the `configuration setter` should be selected. After that the user will be asked to provide the next instructions please. You try with a question **Do we have offers for Software Engineer?** This triggers the Database master, which answers using natural language which brings us to the final state."
   ],
   "metadata": {
    "id": "SFM_-13PvwGl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_state = AgentState(\n",
    "    messages=[HumanMessage(content=\"From now I want to use natural language responses.\", name=\"User\")],\n",
    "    dbally_config=example_dbally_config,\n",
    "    next=\"\",\n",
    ")\n",
    "\n",
    "async for event in app.astream(example_state):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ],
   "metadata": {
    "id": "m7Zz08rRldad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations! Together, we built an agentic system capable of querying the database, changing the configuration dynamically, and incorporating human-in-the-loop. Good job!"
   ],
   "metadata": {
    "id": "Iu_20yZ2IU-f"
   }
  }
 ]
}