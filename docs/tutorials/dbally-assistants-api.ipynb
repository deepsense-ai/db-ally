{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZvWPB4E1fff"
   },
   "source": [
    "# Building AI Agent with OpenAI Assistants API in Streaming mode with access to documents and database\n",
    "\n",
    "In this tutorial, we will build an AI agent for finding the best job offers for candidates from our HR database utilizing [OpenAI Assistants API](https://platform.openai.com/docs/assistants/overview?context=with-streaming). Communication via natural language with the database will be provided by [**db-ally**](https://db-ally.deepsense.ai/) and our final agent will be capable of:\n",
    "1. Listing job offers,\n",
    "2. Matching candidates to offers,\n",
    "3. Drafting an email to the candidate, informing them about job opportunities.\n",
    "\n",
    "At the end of the tutorial, we will be able to have a conversation similar to this one:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1yjzd1z56KwZg-6BsYCeDEIESZEXShE0p\" alt=\"Database schem\" width=\"1000\" height=\"auto\">\n",
    "\n",
    "The architecture of our system looks as follows:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=161znWIfkWe5ehyXyVtmFakxthjyyJFdq\" alt=\"Database schem\" width=\"800\" height=\"auto\">\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install [**db-ally**](https://db-ally.deepsense.ai/) with the OpenAI extension and [nest-asyncio](https://pypi.org/project/nest-asyncio/) needed for nested event-loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwoYXHJY_g3M"
   },
   "outputs": [],
   "source": [
    "!pip install dbally[litellm] nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM68bRgy_hKQ"
   },
   "source": [
    "We start by defining the `OpenAI client`.\n",
    "\n",
    "Remember to set your `OPENAI_API_KEY`. At any point if something is not working always start by checking if this variable is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0VaQtI_-7wO"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n6E2cr1A_uU"
   },
   "source": [
    "Next, we write a function that will create our [Assistant](https://platform.openai.com/docs/assistants/overview?context=with-streaming) with the provided instruction, [tools](https://platform.openai.com/docs/assistants/tools), name, and LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7hD25Mx_Z7V"
   },
   "outputs": [],
   "source": [
    "from openai.types.beta import Assistant\n",
    "\n",
    "\n",
    "def get_assistant(\n",
    "    instructions: str, tools: list[dict] = [], name=\"HR assistant\", model=\"gpt-4-turbo-preview\"\n",
    ") -> Assistant:\n",
    "    return OPENAI_CLIENT.beta.assistants.create(\n",
    "        name=name,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        model=model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeOwdvHoA3Gn"
   },
   "source": [
    "From the perspective of this tutorial two parameters have the biggest influence:\n",
    "* `instructions` - which play a role similar to the [system prompt](https://docs.anthropic.com/claude/docs/system-prompts).\n",
    "* [`tools`](https://platform.openai.com/docs/assistants/tools) -  a list containing external functions defined using JSON format that can be used by the Assistant. Currently, Assistants API supports [code-interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter), [knowledge retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval), and [function calling](https://platform.openai.com/docs/guides/function-calling).\n",
    "\n",
    "Let's inspect the returned object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D-uXetsArX9"
   },
   "outputs": [],
   "source": [
    "BASIC_INSTRUCTION = \"You are a helpful AI assistant that helps in finding jobs and matching candidates to them.\"\n",
    "first_assistant = get_assistant(BASIC_INSTRUCTION)\n",
    "first_assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoPH872PFVU2"
   },
   "source": [
    "Next, we move on and define a chat function that initializes a conversation with an agent. To do so, we need:\n",
    "* a [Thread](https://platform.openai.com/docs/api-reference/threads/object) representing our conversation,\n",
    "* [Messages](https://platform.openai.com/docs/api-reference/messages/object),\n",
    "* and a [Run](https://platform.openai.com/docs/api-reference/runs/object).\n",
    "\n",
    "The code below does three things:\n",
    "1. Defines an Event handler that takes care of every token sent by OpenAI API. It is required for the [streaming mode](https://community.openai.com/t/what-is-this-new-streaming-parameter/391558/2).\n",
    "2. Creates components necessary for interaction with the Assistant.\n",
    "3. Initializes and streams our run inside a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4RtYevoBHJR"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "\n",
    "    def on_tool_call_done(self, tool_call):\n",
    "        print(tool_call, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "def chat(assistant: Assistant, handler_builder: Callable, questions_to_ask: list[str] = []):\n",
    "    thread = OPENAI_CLIENT.beta.threads.create()\n",
    "    questions_gen = (msg for msg in questions_to_ask)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            msg = next(questions_gen)\n",
    "            print(f\"User: {msg}\")\n",
    "        except StopIteration:\n",
    "            msg = input(\"\\nProvide next message or 'q' to finish the conversation: \")\n",
    "\n",
    "        if msg.lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        OPENAI_CLIENT.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=msg)\n",
    "\n",
    "        handler = handler_builder()\n",
    "        handler.thread_id = thread.id\n",
    "        with OPENAI_CLIENT.beta.threads.runs.create_and_stream(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            event_handler=handler,\n",
    "        ) as stream:\n",
    "            stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rREY8AOa9D_"
   },
   "source": [
    "Let's test the initial version of our Assistant. Unfortunately, without any tools, the assistant will not list offers but guide us on how we can find them.\n",
    "\n",
    "**Remark:** At any moment in this tutorial feel free to modify `questions_to_ask` function attribute. Or set it to be an empty array for completely interactive conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e3LeltLLy7V"
   },
   "outputs": [],
   "source": [
    "chat(\n",
    "    first_assistant,\n",
    "    lambda: EventHandler(),\n",
    "    questions_to_ask=[\"What jobs are available for Software Engineer right now? Please be concise.\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtNHjr4AbkbA"
   },
   "source": [
    "The agent needs some refinement. Our next task is to allow the agent to access the recruitment database.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1A5yPt-pIyXGV94c6cMIkMf8AhiP6Nnq6\" alt=\"Database schem\" width=\"500\" height=\"auto\">\n",
    "\n",
    "First, download [`the database`](https://drive.google.com/file/d/1A5yPt-pIyXGV94c6cMIkMf8AhiP6Nnq6/view?usp=drive_link) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X2tJYSrL0qY"
   },
   "outputs": [],
   "source": [
    "!wget -O recruitment.db 'https://drive.google.com/uc?export=download&id=1zo3j8x7qH8opTKyQ9qFgRpS3yqU6uTRs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrqN2vvucTDU"
   },
   "source": [
    "And then:\n",
    "1. Create the database (`ENGINE`). Thanks, [SQLite In-memory database](https://www.sqlite.org/inmemorydb.html) we can load the file directly.\n",
    "2. Define ORM Model (`RECRUITMENT_MODEL`) with [sqlalchemy automap](https://docs.sqlalchemy.org/en/20/orm/extensions/automap.html)\n",
    "3. Check that everything went well with [selecting](https://docs.sqlalchemy.org/en/20/tutorial/data_select.html) and printing 5 rows of every table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCNCRQvScOGG"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import select\n",
    "\n",
    "ENGINE = create_engine(\"sqlite:///recruitment.db\")\n",
    "\n",
    "RECRUITMENT_MODEL = automap_base()\n",
    "RECRUITMENT_MODEL.prepare(autoload_with=ENGINE, reflect=True)\n",
    "\n",
    "# Verify that all entires were created\n",
    "stmts = [\n",
    "    select(RECRUITMENT_MODEL.classes.candidate),\n",
    "    select(RECRUITMENT_MODEL.classes.application),\n",
    "    select(RECRUITMENT_MODEL.classes.offer),\n",
    "]\n",
    "\n",
    "with ENGINE.connect() as conn:\n",
    "    for stmt in stmts:\n",
    "        for row in conn.execute(stmt.limit(5)):\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_lLXkcUcq-6"
   },
   "source": [
    "Next, we create 2 [views](https://db-ally.deepsense.ai/concepts/views/) `JobOfferView` and `CandidateView`.\n",
    "\n",
    "Please, recall the [first tutorial](https://colab.research.google.com/drive/1xUblkrUM3dtVJvQiug_IN_MTNX4bUX-4?usp=sharing) if you are unfamiliar with the View concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok6u44LMcXcJ"
   },
   "outputs": [],
   "source": [
    "from dbally import SqlAlchemyBaseView, decorators\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "class JobOfferView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about job offers.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.offer)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_job_offers_by_position(self, position: str) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.offer.position == position\n",
    "\n",
    "\n",
    "class CandidateView(SqlAlchemyBaseView):\n",
    "    \"\"\"\n",
    "    View meant for answering questions about candiates.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_select(self) -> sqlalchemy.Select:\n",
    "        return select(RECRUITMENT_MODEL.classes.candidate)\n",
    "\n",
    "    @decorators.view_filter()\n",
    "    def filter_candidates_by_experience(self, years: int) -> sqlalchemy.ColumnElement:\n",
    "        return RECRUITMENT_MODEL.classes.candidate.years_of_experience >= years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3Nk9c1Ccf9j"
   },
   "source": [
    "Notice, that we've created 2 filters, so db-ally can now sieve job offers for a given position and also return candidates that have more than the given amount of experience.\n",
    "\n",
    "Next, we create a [Collection](https://db-ally.deepsense.ai/concepts/collections/) and register created Views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Js6-nr5fYua"
   },
   "outputs": [],
   "source": [
    "import dbally\n",
    "from dbally.llms.litellm import LiteLLM\n",
    "\n",
    "llm = LiteLLM(api_key=OPENAI_API_KEY)\n",
    "recruitment_db = dbally.create_collection(\"recruitment\", llm)\n",
    "recruitment_db.add(JobOfferView, lambda: JobOfferView(ENGINE))\n",
    "recruitment_db.add(CandidateView, lambda: CandidateView(ENGINE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAcugN2Zvjgl"
   },
   "source": [
    "How about testing the `recruitment_db` Collection?\n",
    "\n",
    "After running the cell below, you should see an `ExecutionResult` object with Software Engineer job offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSbBI1BVv4Pt"
   },
   "outputs": [],
   "source": [
    "await recruitment_db.ask(\"How many Software Engineer offers do we have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2WgMK77g2HK"
   },
   "source": [
    "## Defining assistant with access to tools.\n",
    "\n",
    "We are almost there! Now, we need to inform our assistant about the possibility of using db-ally. We can achieve it by providing it with instructions on how db-ally can be used - we need a tool definition in JSON format compliant with [OpenAI function calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "`OpenAIAdapter` from dbally will help us to achieve this goal. Let's initialize it with the previously created collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78yaKEFkgGPW"
   },
   "outputs": [],
   "source": [
    "from dbally.assistants.openai import OpenAIAdapter\n",
    "from pprint import pprint\n",
    "\n",
    "adapter = OpenAIAdapter(recruitment_db)\n",
    "\n",
    "tool_json = adapter.generate_function_json()\n",
    "tool_json[\"function\"][\"description\"]\n",
    "pprint(tool_json, width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWaJtPVXi9Qc"
   },
   "source": [
    "Remember that:\n",
    "* `description` field contains information on when and how to use db-ally:\n",
    "  * It states that db-ally has access to the views defined in the collection.\n",
    "  * It demonstrates how to ask questions to db-ally.\n",
    "  * It asks to separate questions from different topics into separate function calls. Yes, the Assistant can call one tool multiple times during one step.\n",
    "* Next, it defines `parameters` In this case it is just `query:str`.\n",
    "\n",
    "So we expect GPT to use the function in the following way: `use_dbally(\"Query about job offers\")`\n",
    "\n",
    "Next, we create our second assistant, this time with provide it with tools to call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wLF3GP5mEJ5"
   },
   "outputs": [],
   "source": [
    "second_assistant = get_assistant(BASIC_INSTRUCTION, tools=[tool_json])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tARHU3JyGvL"
   },
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP4_3r5ByIwC"
   },
   "outputs": [],
   "source": [
    "chat(\n",
    "    second_assistant,\n",
    "    lambda: EventHandler(),\n",
    "    questions_to_ask=[\"What jobs are available for Software Engineer right now? Please be concise.\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Yvb40P7mEjX"
   },
   "source": [
    "Notice that the `FunctionToolCall`. Unfortunately, the Assistant will not execute the function by itself. It will return a JSON that can be used to execute a given function.\n",
    "\n",
    "So, the next step is to write code that will call functions returned by GPT.\n",
    "\n",
    "To achieve it we must extend `on_tool_call_done` function. `AssistantEventHandler` receives GPT response token by token. So, the entire function call is available only at the very end of streaming it. Thus, we handle it inside `on_tool_call_done`.\n",
    "\n",
    "Additionally, we need the `ids` of:\n",
    "* Run,\n",
    "* Assistant,\n",
    "* and Thread\n",
    "\n",
    "to respond back with our function results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3HpYqRriHW0"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    def __init__(self, assistant_id: str, functions: dict[str, callable] = {}, adapter=None):\n",
    "        super().__init__()\n",
    "        self.functions = functions\n",
    "        self.adapter = adapter\n",
    "        self.run_id = None\n",
    "        self.thread_id = None\n",
    "        self.assistant_id = assistant_id\n",
    "\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_run_step_created(self, run_step) -> None:\n",
    "        self.run_id = run_step.run_id\n",
    "\n",
    "    def on_tool_call_done(self, tool_call):\n",
    "        if tool_call.type == \"function\":\n",
    "            response = asyncio.run(self.adapter.process_response([tool_call]))\n",
    "            response_parsed_for_gpt = self.adapter.process_functions_execution(response)\n",
    "\n",
    "            function_name = tool_call.function.name\n",
    "            if function_name in self.functions:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                response = self.functions[function_name](**function_args)\n",
    "                response_parsed_for_gpt.append({\"tool_call_id\": tool_call.id, \"output\": response})\n",
    "\n",
    "            with OPENAI_CLIENT.beta.threads.runs.submit_tool_outputs_stream(\n",
    "                thread_id=self.thread_id,\n",
    "                run_id=self.run_id,\n",
    "                tool_outputs=response_parsed_for_gpt,\n",
    "                event_handler=EventHandler(self.thread_id, self.assistant_id),\n",
    "            ) as stream:\n",
    "                stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92thLptyl5Uj"
   },
   "source": [
    "Remember that:\n",
    "1. dbally is an asynchronous library. Therefore, you always need to either `await` calls to it or run them in separate `asyncio.event_loop`.\n",
    "2. After obtaining a response you can process it with the help of `OpenAIAdapter.process_functions_execution`. In return, you get a dictionary that can be sent back to OpenAI Assistant.\n",
    "\n",
    "Finally, we test our agent capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDB6J43mlgKY"
   },
   "outputs": [],
   "source": [
    "handler_builder = lambda: EventHandler(second_assistant.id, adapter=adapter)\n",
    "chat(\n",
    "    second_assistant,\n",
    "    handler_builder,\n",
    "    questions_to_ask=[\n",
    "        \"What jobs are available for Software Engineer right now?\",\n",
    "        \"Is there a candidate with enough experience for the first google job offer?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZaUSBDJVyC8"
   },
   "source": [
    "## Bonus: Adding more functionalities\n",
    "\n",
    "Let's experiment a little bit and add LinkedIn and email APIs to function calling  to build even more complex interactions with our assistant.\n",
    "\n",
    "**Remark: for the need of this tutorial we Mock access to LinkedIn and Gmail APIs as this is not the most crucial part of it. Feel free to replace mocks with real API calls.**\n",
    "\n",
    "As previously we need to create JSON definitions of function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03Ht__1DV0At"
   },
   "outputs": [],
   "source": [
    "def linkedin_api(position: str) -> str:\n",
    "    return \"\"\"\n",
    "      position: Software Engineer\n",
    "      expected_years_of_experience: 10\n",
    "      company: Amazon\n",
    "      salary: $100000\n",
    "\n",
    "      position: Software Engineer\n",
    "      expected_years_of_experience: 2\n",
    "      company: Neftlix\n",
    "      salary: $60000\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "def gmail_api(job_description: str, recipient: str):\n",
    "    return f\"\"\"\n",
    "        Hello {recipient},\n",
    "\n",
    "        We have found a perfect job offer for you {job_description}\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "li_definition = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"linkedin_api\",\n",
    "        \"description\": \"Use it to get most recent job offers from linkedin\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"position\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Position for which you want to get job offers\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "gmail_definition = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"gmail_api\",\n",
    "        \"description\": \"Use it to send email with job offer to the recipient\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"recipient\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"name of the recipient\",\n",
    "                },\n",
    "                \"job_description\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Description of offered job\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "functions = {\"linkedin_api\": linkedin_api, \"gmail_api\": gmail_api}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApEtmlZC03BA"
   },
   "source": [
    "After this, we just provide all the tools to our agent and test them. Feel free to modify `questions_to_ask` and extend the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Zs8X4I-YgtV"
   },
   "outputs": [],
   "source": [
    "second_assistant = get_assistant(BASIC_INSTRUCTION, tools=[tool_json, li_definition, gmail_definition])\n",
    "\n",
    "handler_builder = lambda: EventHandler(second_assistant.id, adapter=adapter, functions=functions)\n",
    "chat(\n",
    "    second_assistant,\n",
    "    handler_builder,\n",
    "    questions_to_ask=[\"Send and email to sarah.carrey@gmail.com about Google Job for Software Enginner\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2l9hwNG1aCo"
   },
   "source": [
    "This is it, you have built an AI agent capable of:\n",
    "1. Listing job offers,\n",
    "2. Matching candidates to offers,\n",
    "3. Drafting an email to the candidate, informing them about job opportunities.\n",
    "\n",
    "Congratulations!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRcF64bFc17ombogD7+K2t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
